# GroqChat

GroqChat 노드는 Groq사의 초고속 LPU 기반 언어 모델을 호출하는 Chat Model 노드입니다. 낮은 지연 시간과 높은 처리량을 기반으로, 빠른 반응이 요구되는 AI 서비스에 적합합니다.

***

#### 주요 기능

* Groq의 초고속 LLM(Gemma, LLaMA 등)과 통신하여 대화 응답 생성
* 실시간 스트리밍 출력 지원 (Streaming 응답)
* 다양한 모델 선택 가능 (Groq API 연동)
* 응답 다양성 조절을 위한 Temperature 설정 제공

<figure><img src="../../../.gitbook/assets/스크린샷 2025-05-12 131647.png" alt=""><figcaption><p>WindyFlo GroqChat</p></figcaption></figure>

#### 입력값 (Inputs)

| 항목                 | 설명                                 | 필수 여부 |
| ------------------ | ---------------------------------- | ----- |
| Connect Credential | Groq API Key (Credential에 등록된 값)   | 필수    |
| Model Name         | 사용할 LLM 이름 (예: llama3-8b-8192 등)   | 필수    |
| Temperature        | 응답 다양성 조절 값 (0.0 \~ 1.0, 기본값: 0.9) | 선택    |

***

#### 파라미터 (Parameters)

| 항목        | 설명                         |
| --------- | -------------------------- |
| Streaming | 실시간 스트리밍 출력 여부 (기본값: true) |

***

#### 출력값 (Outputs)

| 출력 항목    | 설명                       |
| -------- | ------------------------ |
| GroqChat | Groq 모델로부터 생성된 텍스트 응답 반환 |

***

#### 활용 예시

* **초저지연 음성봇/챗봇 시스템**: 실시간 반응이 핵심인 고객 응대 시스템에 적용
* **AI 기반 실시간 코딩 인터뷰 도우미**: LLM 응답 속도를 활용한 코딩 문제 풀이
* **AI 콘텐츠 생성 플랫폼**: 사용자 요청 즉시 콘텐츠 초안을 작성하는 서비스 구성
* **다중 사용자 챗 서비스**: 많은 요청을 빠르게 처리해야 하는 SaaS 환경에서 유리

***

#### 사용 팁

* **속도가 중요한 챗플로우**에는 GroqChat을 우선 적용해 테스트해보는 것이 효과적
* 모델 선택 시 성능/속도 균형 고려 (예: `llama3-8b-8192`, `gemma-7b`)
* 정답률이 중요한 응답일 경우 Temperature를 **0.3\~0.5**로 조정 추천

***

#### 주의사항

* Credential 등록 없이 호출 시 오류 발생
* 모델 이름은 Groq 공식 명칭과 정확히 일치해야 함
* 한국어 성능은 모델별로 상이하므로 반드시 테스트 필요
* 일부 모델은 응답 길이에 제한이 있음 (Max Token 설정 불가)
